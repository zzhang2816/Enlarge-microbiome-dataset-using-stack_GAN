{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "“t1_2.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmWxcSmcANkw"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxRnACyOkWzo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTzsCy0jk9uU"
      },
      "source": [
        "!ls \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvsyutNClZx6"
      },
      "source": [
        "!pip install tensorview"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2rA_HQblkyI"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjUoTuPA23Kz"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzc5LSWbouwb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Activation, Dense, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Conv1D, GRU\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose, UpSampling1D\n",
        "from tensorflow.keras.layers import LeakyReLU, ReLU,LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorview as tv\n",
        "import tensorflow \n",
        "np.random.seed(1)\n",
        "tensorflow.random.set_seed(3)\n",
        "\n",
        "seq_length = 6\n",
        "features_n = 417\n",
        "SHAPE = (seq_length,features_n)\n",
        "hidden_dim = 512\n",
        "\n",
        "def load_datasets(dataset):\n",
        "    newX = np.load(file=dataset + \"_Combine_Matrix.npy\")\n",
        "    label = np.load(file=dataset + \"_Label.npy\")\n",
        "\n",
        "    print('Combine (sizes * features * time points): ', newX.shape)\n",
        "    print('Label size: ', len(label))\n",
        "\n",
        "    return newX, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdGihkKr3dFD"
      },
      "source": [
        "def generator1(inputs,\n",
        "              activation='sigmoid',\n",
        "              labels=None,\n",
        "              codes=None):\n",
        "    #generator1 output: fake sample\n",
        "    #generator1 output size = (SHAPE[0],SHAPE[1])\n",
        "    if codes is not None:\n",
        "        inputs = [inputs, codes]\n",
        "        x = concatenate(inputs, axis=1)\n",
        "    else:\n",
        "        x = inputs\n",
        "    \n",
        "    x = Dense(SHAPE[0]*SHAPE[1])(x)\n",
        "    x = Reshape((SHAPE[0], SHAPE[1]))(x)\n",
        "    x = GRU(834, return_sequences=False, return_state=False,unroll=True)(x)\n",
        "    x = Reshape((6, int(SHAPE[1]/3)))(x)\n",
        "    x = Conv1D(512, 6, 1, \"same\")(x)\n",
        "    x = BatchNormalization(momentum=0.8)(x) \n",
        "    x = Conv1D(417, 6, 1, \"same\")(x)\n",
        "    x = BatchNormalization(momentum=0.8)(x)\n",
        "\n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "        \n",
        "    return Model(inputs, x,  name='gen1')\n",
        "\n",
        "\n",
        "def build_generators(latent_codes, feature0_dim=512):\n",
        "    #output: gen0, gen1\n",
        "    #gen0 output:fake_feature0\n",
        "    #gen0 output size = 512\n",
        "\n",
        "    labels, z0, z1, feature0 = latent_codes\n",
        "\n",
        "    # gen0\n",
        "    inputs = [labels, z0]\n",
        "    x = concatenate(inputs, axis=1)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    fake_feature0 = Dense(feature0_dim, activation='relu')(x)\n",
        "    gen0 = Model(inputs, fake_feature0, name='gen0')\n",
        "\n",
        "    # gen1\n",
        "    gen1 = generator1(feature0, codes=z1)\n",
        "\n",
        "    return gen0, gen1\n",
        "\n",
        "\n",
        "def discriminator1(inputs,z_dim=50):\n",
        "    #output the probability that sample is real\n",
        "    #output z1 reonstruction\n",
        "    x = inputs\n",
        "    x = GRU(832, return_sequences=False, return_state=False,unroll=True, activation=\"relu\")(x)\n",
        "    x = Reshape((13,64))(x)\n",
        "    x = Conv1D(16, 3,2, \"same\")(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv1D(32, 3, 2, \"same\")(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    z1_recon =  Dense(z_dim)(x)\n",
        "    z1_recon = Activation('tanh', name='z1')(z1_recon)\n",
        "\n",
        "    outputs = [outputs, z1_recon]\n",
        "    dis1=Model(inputs, outputs, name='dis1')\n",
        "    return dis1\n",
        "\n",
        "\n",
        "def discriminator0(inputs, z_dim=50):\n",
        "    #output the probability that feature is real\n",
        "    #output z0 reonstruction\n",
        "    x = Dense(72, activation='relu')(inputs)\n",
        "    x = Dense(72, activation='relu')(x)\n",
        "\n",
        "    f0_source = Dense(1)(x)\n",
        "    f0_source = Activation('sigmoid',\n",
        "                           name='feature1_source')(f0_source)\n",
        "\n",
        "    z0_recon = Dense(z_dim)(x) \n",
        "    z0_recon = Activation('tanh', name='z0')(z0_recon)\n",
        "    \n",
        "    discriminator_outputs = [f0_source, z0_recon]\n",
        "    dis0 = Model(inputs, discriminator_outputs, name='dis0')\n",
        "    return dis0\n",
        "\n",
        "\n",
        "def build_encoder(inputs, num_labels, feature0_dim=512):\n",
        "    x, feature0 = inputs\n",
        "\n",
        "    # Encoder0\n",
        "    y = GRU(72, return_sequences=False, return_state=False,unroll=True)(x)\n",
        "    y = Flatten()(y)\n",
        "    feature0_output = Dense(feature0_dim, activation='relu')(y)\n",
        "    enc0 = Model(inputs=x, outputs=feature0_output, name=\"encoder0\")\n",
        "    \n",
        "    # Encoder1\n",
        "    y = Dense(num_labels)(feature0)\n",
        "    labels = Activation('softmax')(y)\n",
        "    enc1 = Model(inputs=feature0, outputs=labels, name=\"encoder1\")\n",
        "\n",
        "    return enc0, enc1\n",
        "\n",
        "\n",
        "def train_encoder(model,\n",
        "                  data, \n",
        "                  model_name=\"MTSS-GAN\", \n",
        "                  batch_size=64):\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = data\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(x_train,\n",
        "              y_train,\n",
        "              validation_split=0.1,\n",
        "              epochs=10,\n",
        "              batch_size=batch_size)\n",
        "\n",
        "    model.save(model_name + \"-encoder.h5\")\n",
        "    score = model.evaluate(x_test,\n",
        "                           y_test, \n",
        "                           batch_size=batch_size,\n",
        "                           verbose=0)\n",
        "    print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvJkO5Qa3dHw"
      },
      "source": [
        "def train(models, data, params):\n",
        "    # the MTSS-GAN and Encoder models\n",
        "\n",
        "    enc0, enc1, gen0, gen1, dis0, dis1, adv0, adv1 = models\n",
        "    # network parameters\n",
        "    batch_size, train_steps, num_labels, z_dim, model_name = params\n",
        "    # train dataset\n",
        "    (x_train, y_train), (_, _) = data # I can do this. \n",
        "    # the generated time series array is saved every 500 steps\n",
        "    save_interval = 500\n",
        "\n",
        "    # label and noise codes for generator testing\n",
        "    z0 = np.random.normal(scale=0.5, size=[SHAPE[0], z_dim])\n",
        "    z1 = np.random.normal(scale=0.5, size=[SHAPE[0], z_dim])\n",
        "    noise_class = np.eye(num_labels)[np.arange(0, SHAPE[0]) % num_labels]\n",
        "    print(noise_class.shape)\n",
        "    noise_params = [noise_class, z0, z1]\n",
        "    # number of elements in train dataset\n",
        "    train_size = x_train.shape[0]\n",
        "    print(model_name,\n",
        "          \"Labels for generated time series arrays: \",\n",
        "          np.argmax(noise_class, axis=1))\n",
        "\n",
        "    tv_plot = tv.train.PlotMetrics(columns=5, wait_num=5)\n",
        "    for i in range(train_steps):\n",
        "        # train the discriminator1 for 1 batch\n",
        "        # 1 batch of real (label=1.0) and fake feature1 (label=0.0)\n",
        "        # randomly pick real time series arrays from dataset\n",
        "        dicta = {}\n",
        "        rand_indexes = np.random.randint(0, \n",
        "                                         train_size, \n",
        "                                         size=batch_size)\n",
        "        real_samples = x_train[rand_indexes]\n",
        "        # real feature1 from encoder0 output\n",
        "        real_feature0 = enc0.predict(real_samples)\n",
        "        # generate random 50-dim z1 latent code\n",
        "        real_z0 = np.random.normal(scale=0.5,\n",
        "                                   size=[batch_size, z_dim])\n",
        "        # real labels from dataset\n",
        "        real_labels = y_train[rand_indexes]\n",
        "\n",
        "        # generate fake feature1 using generator1 from\n",
        "        # real labels and 50-dim z1 latent code\n",
        "        fake_z0 = np.random.normal(scale=0.5,\n",
        "                                   size=[batch_size, z_dim])\n",
        "        fake_feature0 = gen0.predict([real_labels, fake_z0])\n",
        "\n",
        "        # real + fake data\n",
        "        feature0 = np.concatenate((real_feature0, fake_feature0))\n",
        "        z0 = np.concatenate((fake_z0, fake_z0))\n",
        "\n",
        "        # label 1st half as real and 2nd half as fake\n",
        "        y = np.ones([2 * batch_size, 1])\n",
        "        y[batch_size:, :] = 0\n",
        "\n",
        "        # train discriminator1 to classify feature1 as \n",
        "        # real/fake and recover\n",
        "        # latent code (z0). real = from encoder1, \n",
        "        # fake = from genenerator10\n",
        "        # joint training using discriminator part of \n",
        "        # advserial1 loss and entropy0 loss\n",
        "        metrics = dis0.train_on_batch(feature0, [y, z0])\n",
        "        # log the overall loss only\n",
        "        log = \"%d: [dis0_loss: %f]\" % (i, metrics[0])\n",
        "        dicta[\"dis0_loss\"] = metrics[0]\n",
        "         \n",
        "        # train the discriminator1 for 1 batch\n",
        "        # 1 batch of real (label=1.0) and fake time series arrays (label=0.0)\n",
        "        # generate random 50-dim z1 latent code\n",
        "        fake_z1 = np.random.normal(scale=0.5, size=[batch_size, z_dim])\n",
        "        # generate fake time series arrays from real feature1 and fake z1\n",
        "        fake_samples = gen1.predict([real_feature0, fake_z1])\n",
        "       \n",
        "        # real + fake data\n",
        "        x = np.concatenate((real_samples, fake_samples))\n",
        "        z1 = np.concatenate((fake_z1, fake_z1))\n",
        "\n",
        "        # train discriminator1 to classify time series arrays \n",
        "        # as real/fake and recover latent code (z1)\n",
        "        # joint training using discriminator part of advserial0 loss\n",
        "        # and entropy1 loss\n",
        "        metrics = dis1.train_on_batch(x, [y, z1])\n",
        "        # log the overall loss only (use dis1.metrics_names)\n",
        "        log = \"%s [dis1_loss: %f]\" % (log, metrics[0])\n",
        "        dicta[\"dis1_loss\"] = metrics[0]\n",
        "\n",
        "        # adversarial training \n",
        "        # generate fake z0, labels\n",
        "        fake_z0 = np.random.normal(scale=0.5, \n",
        "                                   size=[batch_size, z_dim])\n",
        "        # input to generator0 is sampling fr real labels and\n",
        "        # 50-dim z0 latent code\n",
        "        gen0_inputs = [real_labels, fake_z0]\n",
        "\n",
        "        # label fake feature0 as real (specifies whether real or not)\n",
        "        # is it bypassing the discriminator?\n",
        "        y = np.ones([batch_size, 1])\n",
        "    \n",
        "        # train generator0 (thru adversarial) by fooling \n",
        "        # the discriminator\n",
        "        # and approximating encoder1 feature0 generator\n",
        "        # joint training: adversarial0, entropy0, conditional0\n",
        "        metrics = adv0.train_on_batch(gen0_inputs,\n",
        "                                      [y, fake_z0, real_labels])\n",
        "        fmt = \"%s [adv0_loss: %f, enc1_acc: %f]\"\n",
        "        dicta[\"adv0_loss\"] = metrics[0]\n",
        "        dicta[\"enc1_acc\"] = metrics[6]\n",
        "\n",
        "        # log the overall loss and classification accuracy\n",
        "        log = fmt % (log, metrics[0], metrics[6])\n",
        "\n",
        "        # input to generator0 is real feature0 and \n",
        "        # 50-dim z0 latent code\n",
        "        fake_z1 = np.random.normal(scale=0.5,\n",
        "                                   size=[batch_size, z_dim])\n",
        "        \n",
        "        gen1_inputs = [real_feature0, fake_z1]\n",
        "\n",
        "        # train generator1 (thru adversarial) by fooling \n",
        "        # the discriminator and approximating encoder1 time series arrays \n",
        "        # source generator joint training: \n",
        "        # adversarial1, entropy1, conditional1\n",
        "        metrics = adv1.train_on_batch(gen1_inputs,\n",
        "                                      [y, fake_z1, real_feature0])\n",
        "        # log the overall loss only\n",
        "        log = \"%s [adv1_loss: %f]\" % (log, metrics[0])\n",
        "        dicta[\"adv1_loss\"] = metrics[0]\n",
        "\n",
        "\n",
        "        print(log)\n",
        "        # if (i + 1) % save_interval == 0:\n",
        "        #     generators = (gen0, gen1)\n",
        "        #     plot_ts(generators,\n",
        "        #                 noise_params=noise_params,\n",
        "        #                 show=False,\n",
        "        #                 step=(i + 1),\n",
        "        #                 model_name=model_name)\n",
        "            \n",
        "        tv_plot.update({'dis0_loss': dicta[\"dis0_loss\"], 'dis1_loss': dicta[\"dis1_loss\"], 'adv0_loss': dicta[\"adv0_loss\"], 'enc1_acc': dicta[\"enc1_acc\"], 'adv1_loss': dicta[\"adv1_loss\"]})\n",
        "        tv_plot.draw()\n",
        "\n",
        "    # save the modelis after training generator0 & 1\n",
        "    # the trained generator can be reloaded for\n",
        "    # future data generation\n",
        "    gen0.save(model_name + \"-gen0.h5\")\n",
        "    gen1.save(model_name + \"-gen1.h5\")\n",
        "\n",
        "    return  gen0, gen1 \n",
        "    \n",
        "\n",
        "def plot_ts(generators,\n",
        "                noise_params,\n",
        "                show=False,\n",
        "                step=0,\n",
        "                model_name=\"gan\"):\n",
        "    \"\"\"Generate fake time series arrays and plot them\n",
        "    For visualization purposes, generate fake time series arrays\n",
        "    then plot them in a square grid\n",
        "    # Arguments\n",
        "        generators (Models): gen0 and gen1 models for \n",
        "            fake time series arrays generation\n",
        "        noise_params (list): noise parameters \n",
        "            (label, z0 and z1 codes)\n",
        "        show (bool): Whether to show plot or not\n",
        "        step (int): Appended to filename of the save time series arrays\n",
        "        model_name (string): Model name\n",
        "    \"\"\"\n",
        "\n",
        "    gen0, gen1 = generators\n",
        "    noise_class, z0, z1 = noise_params\n",
        "    feature0 = gen0.predict([noise_class, z0])\n",
        "    tss = gen1.predict([feature0, z1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmxBPrri3dKh"
      },
      "source": [
        "def build_and_train_models(train_steps,data):\n",
        "    (x_train, y_train), (x_test, y_test)=data\n",
        "    \n",
        "    num_labels = len(np.unique(y_train))\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "    data=(x_train, y_train), (x_test, y_test)\n",
        "    model_name = \"MTSS-GAN\"\n",
        "    batch_size = 64\n",
        "    lr = 2e-4\n",
        "    decay = 6e-8\n",
        "    z_dim = 50 ##this is the noise input\n",
        "    z_shape = (z_dim, )\n",
        "    feature0_dim = 512\n",
        "    feature0_shape = (feature0_dim, )\n",
        "    optimizer = RMSprop(lr=lr, decay=decay)\n",
        "\n",
        "    # build discriminator 0 and Q network 0 models\n",
        "    input_shape = (feature0_dim, )\n",
        "    inputs = Input(shape=input_shape, name='dis0_input')\n",
        "    dis0 = discriminator0(inputs, z_dim=z_dim )\n",
        "\n",
        "    loss = ['binary_crossentropy', 'mse']\n",
        "    loss_weights = [1.0, 1.0] \n",
        "    dis0.compile(loss=loss,\n",
        "                 loss_weights=loss_weights,\n",
        "                 optimizer=optimizer,\n",
        "                 metrics=['accuracy'])\n",
        "    dis0.summary() \n",
        "\n",
        "    # build discriminator 1 and Q network 1 models\n",
        "    input_shape = (SHAPE[0],SHAPE[1])\n",
        "    inputs = Input(shape=input_shape, name='dis1_input')\n",
        "    dis1 = discriminator1(inputs, z_dim=z_dim)\n",
        "\n",
        "    loss = ['binary_crossentropy', 'mse']\n",
        "    loss_weights = [1.0, 10.0] ###? why is 10\n",
        "    dis1.compile(loss=loss,\n",
        "                 loss_weights=loss_weights,\n",
        "                 optimizer=optimizer,\n",
        "                 metrics=['accuracy'])\n",
        "    dis1.summary() \n",
        "\n",
        "    # build generator models\n",
        "    label_shape = (num_labels, )\n",
        "    feature0 = Input(shape=feature0_shape, name='feature0_input')\n",
        "    labels = Input(shape=label_shape, name='labels')\n",
        "    z0 = Input(shape=z_shape, name=\"z0_input\")\n",
        "    z1 = Input(shape=z_shape, name=\"z1_input\")\n",
        "    latent_codes = (labels, z0, z1, feature0)\n",
        "    gen0, gen1 = build_generators(latent_codes)\n",
        "    gen0.summary()\n",
        "    gen1.summary()\n",
        "\n",
        "    # build encoder models\n",
        "    input_shape = SHAPE\n",
        "    inputs = Input(shape=input_shape, name='encoder_input')\n",
        "    enc0, enc1 = build_encoder((inputs, feature0), num_labels)\n",
        "    enc0.summary() \n",
        "    enc1.summary() \n",
        "    encoder = Model(inputs, enc1(enc0(inputs)))\n",
        "    encoder.summary() \n",
        "\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "    train_encoder(encoder, data, model_name=model_name)\n",
        "\n",
        "\n",
        "    # build adversarial0 model = \n",
        "    # generator0 + discriminator0 + encoder1\n",
        "    # encoder0 weights frozen\n",
        "    enc1.trainable = False\n",
        "    dis0.trainable = False\n",
        "    gen0_inputs = [labels, z0]\n",
        "    gen0_outputs = gen0(gen0_inputs)\n",
        "    adv0_outputs = dis0(gen0_outputs) + [enc1(gen0_outputs)]\n",
        "    adv0 = Model(gen0_inputs, adv0_outputs, name=\"adv0\")\n",
        "    \n",
        "    loss_weights = [1.0, 1.0, 1.0] \n",
        "    loss = ['binary_crossentropy', \n",
        "            'mse',\n",
        "            'categorical_crossentropy']\n",
        "    adv0.compile(loss=loss,\n",
        "                 loss_weights=loss_weights,\n",
        "                 optimizer=optimizer,\n",
        "                 metrics=['accuracy'])\n",
        "    adv0.summary()\n",
        "\n",
        "    # build adversarial1 model =\n",
        "    # generator1 + discriminator1 + encoder0\n",
        "    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)\n",
        "    enc0.trainable = False\n",
        "    dis1.trainable = False\n",
        "    gen1_inputs = [feature0, z1]\n",
        "    gen1_outputs = gen1(gen1_inputs)\n",
        "    print(gen1_inputs)\n",
        "    print(gen1_outputs)\n",
        "    adv1_outputs = dis1(gen1_outputs)\n",
        "    adv1_outputs +=  [enc0(gen1_outputs)]\n",
        "    adv1 = Model(gen1_inputs, adv1_outputs, name=\"adv1\")\n",
        "    loss = ['binary_crossentropy', 'mse', 'mse']\n",
        "    loss_weights = [1.0, 10.0, 1.0] ##?why 10\n",
        "    adv1.compile(loss=loss,\n",
        "                 loss_weights=loss_weights,\n",
        "                 optimizer=optimizer,\n",
        "                 metrics=['accuracy'])\n",
        "    adv1.summary()\n",
        "\n",
        "    # train discriminator and adversarial networks\n",
        "    models = (enc0, enc1, gen0, gen1, dis0, dis1, adv0, adv1)\n",
        "    params = (batch_size, train_steps, num_labels, z_dim, model_name)\n",
        "    gen0, gen1 = train(models, data, params)\n",
        "\n",
        "    return gen0, gen1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeU7lQRp3dM-"
      },
      "source": [
        "def lstm_model(num_time_step, dim, num_features,model_name=\"lstm\"):\n",
        "    X = Input(shape=(num_time_step,num_features))  #ndedit\n",
        "    Y = LSTM(units=dim, return_sequences=False)(X)\n",
        "    Y = Dense(units=128)(Y)  \n",
        "    Y = Activation('relu')(Y)\n",
        "    Y = Dense(units=1)(Y)\n",
        "    Y = Activation('sigmoid')(Y)\n",
        "    model = Model(inputs=X, outputs=Y)\n",
        "    return model\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "from sklearn.metrics import roc_auc_score,f1_score\n",
        "def build_and_train_lstm_models(data):\n",
        "    (x_train, y_train), (x_test, y_test)=data\n",
        "    ### build the lstm model ###\n",
        "    n_a = 128 \n",
        "    n_features = 417  \n",
        "    n_t= 6   \n",
        "    model = lstm_model(n_t, n_a, n_features)\n",
        "    opt = SGD(learning_rate=0.1, momentum=0.1)\n",
        "\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    callback = EarlyStopping(monitor='val_loss',min_delta=0.001,patience=10,restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                                patience=5, min_lr=0.001)  \n",
        "\n",
        "    ### train the model\n",
        "    tprs = []\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    fig, ax = plt.subplots()\n",
        "    model.fit(x_train,y_train, epochs=300, shuffle=True,callbacks=[callback,reduce_lr],validation_split=0.1)\n",
        "\n",
        "    y_pred_prob = model.predict(x_test)\n",
        "\n",
        "    y_pred_label = (y_pred_prob>[0.5]).astype(int)\n",
        "    f1_test=f1_score(y_true=y_test, y_pred=y_pred_label)\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    interp_tpr = np.interp(mean_fpr, fpr, tpr) \n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "\n",
        "    print(\"f1_pred:\",f1_test)\n",
        "    print(\"auc_pred\",auc)\n",
        "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "            label='Chance', alpha=.8)\n",
        "    mean_tpr = np.mean(tprs, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
        "            label=r'Mean ROC (AUC = %0.2f)' % (auc),\n",
        "            lw=2, alpha=.8)\n",
        "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
        "        title=\"ROC figure\")\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    ### clear the graph\n",
        "    K.clear_session() \n",
        "    return auc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY0coJb33dQG"
      },
      "source": [
        "###  train the gan model  ###\n",
        "steps = 2000\n",
        "data_x, data_y = load_datasets('knat')\n",
        "data_x = data_x.transpose(0, 2, 1) \n",
        "data_size=data_x.shape[0]\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data_x, data_y, test_size=0.2)\n",
        "data=(x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# gen0, gen1 = build_and_train_models(train_steps = steps,data=data)\n",
        "gen1=load_model(\"MTSS-GAN-gen1.h5\")\n",
        "gen0=load_model(\"MTSS-GAN-gen0.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHJd9jh7rEtW"
      },
      "source": [
        "x=data_x[0,:,:]\n",
        "plt.plot(x)\n",
        "plt.show()\n",
        "x=gen_data[14,:,:]\n",
        "plt.plot(x)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtJM-OVy0J0c",
        "scrolled": true
      },
      "source": [
        "### base case ###\n",
        "aucs=[]\n",
        "auc=build_and_train_lstm_models(data)\n",
        "aucs.append(auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br4A4W7H3dA3"
      },
      "source": [
        "def generate_data(percent):\n",
        "    ###  generate the new data  ###\n",
        "    z_dim=50\n",
        "    num_labels=2\n",
        "    nums=(int)(data_size*percent/2)*2\n",
        "    z0 = np.random.normal(scale=0.5, size=[nums,z_dim])\n",
        "    z1 = np.random.normal(scale=0.5, size=[nums,z_dim])\n",
        "    nums=(int)(nums/2)\n",
        "    y_pos=np.zeros((nums,), dtype=int)\n",
        "    y_neg=np.ones((nums,), dtype=int)\n",
        "    y_gan=np.concatenate((y_pos,y_neg))\n",
        "    noise_class = np.eye(num_labels)[y_gan]\n",
        "    feature0 = gen0.predict([noise_class, z0])\n",
        "    gen_data = gen1.predict([feature0, z1])\n",
        "    return gen_data,y_gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PECW9EMi0J3F",
        "scrolled": true
      },
      "source": [
        "### concatenate the orignal train/val data and generate data ###\n",
        "for i in range(10):\n",
        "    gen_data,y_gan=generate_data(0.1)\n",
        "    x_train=np.concatenate((gen_data,x_train))\n",
        "    y_train=np.concatenate((y_gan,y_train))\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "    data=(x_train, y_train), (x_test, y_test)\n",
        "    ### train and val using generate data\n",
        "    auc=build_and_train_lstm_models(data)\n",
        "    aucs.append(auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o96aCeDEBENF"
      },
      "source": [
        "x=np.linspace(0,1,11)\n",
        "plt.plot(x,aucs)\n",
        "print(x)\n",
        "print(aucs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZraUM88BENG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6xhcM1GBENG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXOb9FndBENG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}